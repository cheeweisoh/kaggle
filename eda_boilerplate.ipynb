{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
    "\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#2E3440\",\n",
    "    \"figure.facecolor\": \"#2E3440\",\n",
    "    \"axes.edgecolor\": \"#4C566A\",\n",
    "    \"grid.color\": \"#4C566A\",\n",
    "    \"font.family\": \"JetBrains Mono\",\n",
    "    \"text.color\": \"#FFFFFF\",\n",
    "    \"axes.labelcolor\": \"#FFFFFF\",\n",
    "    \"xtick.color\": \"#FFFFFF\",\n",
    "    \"ytick.color\": \"#FFFFFF\",\n",
    "    \"grid.alpha\": 0.4,\n",
    "    \"font.size\": 10,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"legend.title_fontsize\": 10\n",
    "}\n",
    "\n",
    "bp_props = {\n",
    "    'boxprops':{'facecolor':'#037d97', 'edgecolor':'white'},\n",
    "    'medianprops':{'color':'white'},\n",
    "    'whiskerprops':{'color':'white'},\n",
    "    'capprops':{'color':'white'},\n",
    "    'flierprops':{'marker':'x', 'markeredgecolor':'white'}\n",
    "}\n",
    "\n",
    "sns.set(rc=rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col='id')\n",
    "test_df = pd.read_csv('test.csv', index_col='id')\n",
    "\n",
    "orig_df = pd.read_csv('original.csv')\n",
    "\n",
    "target_var = ''\n",
    "cat_features = []\n",
    "num_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Descriptors')\n",
    "print(f'Shape: {train_df.shape}')\n",
    "print(f'Columns : {train_df.columns}')\n",
    "print('')\n",
    "print('Test Descriptors')\n",
    "print(f'Shape: {test_df.shape}')\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = {'column': [], 'train_null_values': [], 'test_null_values': []}\n",
    "\n",
    "for col in test_df.columns:\n",
    "    train_col_null = train_df[col].isna().sum()\n",
    "    test_col_null = test_df[col].isna().sum()\n",
    "    \n",
    "    null_values['column'].append(col)\n",
    "    null_values['train_null_values'].append(train_col_null)\n",
    "    null_values['test_null_values'].append(test_col_null)\n",
    "    \n",
    "null_values = pd.DataFrame(null_values)\n",
    "null_values['train_total'] = train_df.shape[0]\n",
    "null_values['test_total'] = test_df.shape[0]\n",
    "null_values['train_null_perc'] = null_values['train_null_values'] / null_values['train_total']\n",
    "null_values['test_null_perc'] = null_values['test_null_values'] / null_values['test_total']\n",
    "null_values.sort_values(by='train_null_perc', inplace=True, ascending=False)\n",
    "\n",
    "null_values.style.format({'train_null_perc': '{:.0%}',\n",
    "                          'test_null_perc': '{:.0%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_per_row = train_df.isna().sum(axis=1)\n",
    "pd.DataFrame(null_values_per_row.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = {'column': [], 'only_in_train': [], 'only_in_test': []}\n",
    "\n",
    "for col in cat_features:\n",
    "    train_col_unique = train_df[col].unique()\n",
    "    test_col_unique = test_df[col].unique()\n",
    "    \n",
    "    missing_values['column'].append(col)\n",
    "    missing_values['only_in_train'].append([f for f in train_col_unique if f not in test_col_unique])\n",
    "    missing_values['only_in_test'].append([f for f in test_col_unique if f not in train_col_unique])\n",
    "\n",
    "missing_values = pd.DataFrame(missing_values)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train vs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[num_features].assign(source=0), test_df[num_features].assign(source=1)], ignore_index=True)\n",
    "origin_var = df['source']\n",
    "df.drop(columns='source', inplace=True)\n",
    "\n",
    "n_folds = 5\n",
    "k_fold = KFold(n_splits=n_folds, random_state=2023, shuffle=True)\n",
    "train_oof_preds = np.zeros((df.shape[0],))\n",
    "train_oof_probas = np.zeros((df.shape[0],))\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(k_fold.split(df, origin_var)):\n",
    "    print('-----> Fold {} <-----'.format(fold+1))\n",
    "    X_train, X_valid = pd.DataFrame(df.iloc[train_index]), pd.DataFrame(df.iloc[test_index])\n",
    "    y_train, y_valid = origin_var.iloc[train_index], origin_var.iloc[test_index]\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        random_state=2023,\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        n_jobs=-1,\n",
    "        n_estimators=2000,\n",
    "        verbose=-1,\n",
    "        max_depth=3,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[\n",
    "            early_stopping(50, verbose=False),\n",
    "            log_evaluation(2000),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    oof_preds = model.predict(X_valid)\n",
    "    oof_probas = model.predict_proba(X_valid)[:,1]\n",
    "    train_oof_preds[test_index] = oof_preds\n",
    "    train_oof_probas[test_index] = oof_probas\n",
    "    print(': AUC ROC = {}'.format(roc_auc_score(y_valid, oof_probas)))\n",
    "\n",
    "auc_vanilla = roc_auc_score(origin_var, train_oof_probas)\n",
    "fpr, tpr, _ = roc_curve(origin_var, train_oof_probas)\n",
    "print(\"--> Overall results for out of fold predictions\")\n",
    "print(\": AUC ROC = {}\".format(auc_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(origin_var, train_oof_preds)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
    "\n",
    "sns.heatmap(confusion, annot=True, fmt=\",d\", cmap='RdBu', ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix (@ 0.5 Probability)\")\n",
    "axs[0].set_ylabel(\"Actual Class\")\n",
    "axs[0].set_xlabel(\"Predicted Class\")\n",
    "\n",
    "sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\", ax=axs[1])\n",
    "sns.lineplot(x=fpr, y=tpr, ax=axs[1], label=\"Adversarial Validation Classifier\")\n",
    "axs[1].set_title(\"ROC Curve\")\n",
    "axs[1].set_xlabel(\"False Positive Rate\")\n",
    "axs[1].set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[num_features].assign(source=0), test_df[num_features].assign(source=0), orig_df[num_features].assign(source=1)], ignore_index=True)\n",
    "origin_var = df['source']\n",
    "df.drop(columns='source', inplace=True)\n",
    "\n",
    "n_folds = 5\n",
    "k_fold = KFold(n_splits=n_folds, random_state=2023, shuffle=True)\n",
    "train_oof_preds = np.zeros((df.shape[0],))\n",
    "train_oof_probas = np.zeros((df.shape[0],))\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(k_fold.split(df, origin_var)):\n",
    "    print('-----> Fold {} <-----'.format(fold+1))\n",
    "    X_train, X_valid = pd.DataFrame(df.iloc[train_index]), pd.DataFrame(df.iloc[test_index])\n",
    "    y_train, y_valid = origin_var.iloc[train_index], origin_var.iloc[test_index]\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        random_state=2023,\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        n_jobs=-1,\n",
    "        n_estimators=2000,\n",
    "        verbose=-1,\n",
    "        max_depth=3,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[\n",
    "            early_stopping(50, verbose=False),\n",
    "            log_evaluation(2000),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    oof_preds = model.predict(X_valid)\n",
    "    oof_probas = model.predict_proba(X_valid)[:,1]\n",
    "    train_oof_preds[test_index] = oof_preds\n",
    "    train_oof_probas[test_index] = oof_probas\n",
    "    print(': AUC ROC = {}'.format(roc_auc_score(y_valid, oof_probas)))\n",
    "\n",
    "auc_vanilla = roc_auc_score(origin_var, train_oof_probas)\n",
    "fpr, tpr, _ = roc_curve(origin_var, train_oof_probas)\n",
    "print(\"--> Overall results for out of fold predictions\")\n",
    "print(\": AUC ROC = {}\".format(auc_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(origin_var, train_oof_preds)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
    "\n",
    "sns.heatmap(confusion, annot=True, fmt=\",d\", cmap='RdBu', ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix (@ 0.5 Probability)\")\n",
    "axs[0].set_ylabel(\"Actual Class\")\n",
    "axs[0].set_xlabel(\"Predicted Class\")\n",
    "\n",
    "sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\", ax=axs[1])\n",
    "sns.lineplot(x=fpr, y=tpr, ax=axs[1], label=\"Adversarial Validation Classifier\")\n",
    "axs[1].set_title(\"ROC Curve\")\n",
    "axs[1].set_xlabel(\"False Positive Rate\")\n",
    "axs[1].set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train_df, x='outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[test_df.columns], test_df], ignore_index=True)\n",
    "df_corr = df.corr(method='pearson')\n",
    "mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df_corr, mask=mask, annot=True, annot_kws={\"size\": 7}, fmt='.2f', cmap='RdBu', square=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=len(cat_features)//3 + 1, ncols=3, figsize=(30,30))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, feat in enumerate(cat_features):\n",
    "    sns.countplot(data=train_df, x=feat, ax=axs[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df[num_features].assign(source='train'), test_df[num_features].assign(source='test')], ignore_index=True)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(num_features), ncols=3, figsize=(16, len(num_features) * 3),\n",
    "                        gridspec_kw= {'hspace': 0.35, 'wspace': 0.3, 'width_ratios': [0.8, 0.2, 0.2]})\n",
    "\n",
    "for i, feat in enumerate(num_features):\n",
    "    sns.kdeplot(data=df[[feat, 'source']], x=feat, hue='source', linewidth=2, ax=axs[i, 0])\n",
    "    axs[i, 0].set_title(feat, fontsize=9)\n",
    "    axs[i, 0].set(xlabel='', ylabel='')\n",
    "    \n",
    "    sns.boxplot(data=df.loc[df['source'] == 'train', [feat]], y=feat, width=0.25, saturation=0.9, linewidth=0.9, fliersize=2.25, ax=axs[i, 1], **bp_props)\n",
    "    axs[i, 1].set_title('train', fontsize=9)\n",
    "    axs[i, 1].set(xlabel='', ylabel='')\n",
    "    \n",
    "    sns.boxplot(data=df.loc[df['source'] == 'test', [feat]], y=feat, width=0.25, saturation=0.9, linewidth=0.9, fliersize=2.25, color='#037d97', ax=axs[i, 2], **bp_props)\n",
    "    axs[i, 2].set_title('test', fontsize=9, fontweight='bold')\n",
    "    axs[i, 2].set(xlabel='', ylabel='')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsolationForest(contamination='auto', random_state=2023)\n",
    "outlier_preds = iso.fit_predict(train_df)\n",
    "outlier_df = train_df[outlier_preds == -1]\n",
    "\n",
    "print(f'Number of Outliers: {outlier_df.shape[0]}')\n",
    "print(f'Percentage of Rows: {round(100 * outlier_df.shape[0] / train_df.shape[0], 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_df = train_df.copy()\n",
    "\n",
    "combined_df = pd.concat([cleaned_train_df, test_df], ignore_index=True)\n",
    "\n",
    "cleaned_train_df = combined_df[~combined_df[target_var].isnull()]\n",
    "cleaned_test_df = combined_df[combined_df[target_var].isnull()].drop(target_var, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Fold Training\n",
    "n_folds = 5\n",
    "k_fold = KFold(n_splits=n_folds, random_state=2023, shuffle=True)\n",
    "train_oof_preds = np.zeros((cleaned_train_df.shape[0],))\n",
    "oof_metric = []\n",
    "\n",
    "final_train_df = cleaned_train_df.drop(target_var, axis=1)\n",
    "target = cleaned_train_df[target_var]\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(k_fold.split(final_train_df, target)):\n",
    "    X_train, X_valid = final_train_df.iloc[train_idx], final_train_df.iloc[test_idx]\n",
    "    y_train, y_valid = target.iloc[train_idx], target.iloc[test_idx]\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=10,\n",
    "        random_state=2023,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=(X_valid, y_valid),\n",
    "        eval_metric='auc',\n",
    "        callbacks=[\n",
    "            log_evaluation(0),\n",
    "            early_stopping(50, verbose=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    oof_preds = model.predict_proba(X_valid)[:, 1]\n",
    "    train_oof_preds[test_idx] = oof_preds\n",
    "    oof_metric.append(roc_auc_score(y_valid, oof_preds))\n",
    "    \n",
    "oof_metric = np.array(oof_metric)\n",
    "overall_metric = roc_auc_score(target, train_oof_preds)\n",
    "print(f'OOF Scores: {oof_metric}')\n",
    "print(f'Mean Score: {oof_metric.mean()}')\n",
    "print(f'Standard Deviation: {oof_metric.std()}')\n",
    "print('')\n",
    "print(f'Overall Result: {overall_metric}')\n",
    "print('')\n",
    "print(f'{oof_metric}, {oof_metric.mean()}, {oof_metric.std()}, {overall_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=10,\n",
    "        random_state=2023,\n",
    "        verbose=-1\n",
    "    )\n",
    "model.fit(cleaned_train_df.drop(target_var, axis=1), cleaned_train_df[target_var])\n",
    "\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feat': cleaned_train_df.drop(target_var, axis=1).columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 0.2*len(feat_imp)))\n",
    "sns.barplot(data=feat_imp, x='importance', y='feat')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict_proba(cleaned_test_df)[:, 1]\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['smoking'] = test_preds\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
